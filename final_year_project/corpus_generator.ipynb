{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pool_location = \"corpus/corpus_pool.txt\"\n",
    "fopen = open(corpus_pool_location, \"w\")\n",
    "fopen.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get corpus for BBC-News and BBC-Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']\n",
    "\n",
    "rx = r'\\s*[!?\\.]\\s*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bbc = \"corpus/bbc/\"\n",
    "source_bbcsport = \"corpus/bbcsport\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_store(location):\n",
    "    try:\n",
    "        fopen = codecs.open(location, encoding='utf-8')\n",
    "        data = fopen.read()\n",
    "        result = re.sub(rx, \".\\n\", data)\n",
    "        fopen.close()\n",
    "        \n",
    "        fopen = open(corpus_pool_location, \"a\")\n",
    "        result = result.split(\"\\n\")\n",
    "        for k in range(len(result)):\n",
    "            if len(result[k]) > 50:\n",
    "                for l in range(len(punct)):\n",
    "                    result[k] = result[k].replace(punct[l], '')\n",
    "                fopen.write(result[k] + \"\\n\")\n",
    "        fopen.close()\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_storeuci(location):\n",
    "    try:\n",
    "        fopen = codecs.open(location, encoding='utf-8')\n",
    "        data = fopen.read()\n",
    "        data = data.split(\"\\n\")\n",
    "        data = data[1:]\n",
    "        datum = \"\"\n",
    "        for k in range(len(data)):\n",
    "            datum += data[k]\n",
    "        datum = datum[9:]\n",
    "        result = re.sub(rx, \".\\n\", datum)\n",
    "        fopen.close()\n",
    "        \n",
    "        fopen = open(corpus_pool_location, \"a\")\n",
    "        result = result.split(\"\\n\")\n",
    "        for k in range(len(result)):\n",
    "            if len(result[k]) > 50:\n",
    "                for l in range(len(punct)):\n",
    "                    result[k] = result[k].replace(punct[l], '')\n",
    "                fopen.write(result[k] + \"\\n\")\n",
    "        fopen.close()\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getall_subdir(path):\n",
    "    subdir = os.listdir(path)\n",
    "    for k in range(len(subdir)):\n",
    "        subpath = path + subdir[k] + \"/\"\n",
    "        name_allfile = glob.glob(subpath + \"*.txt\")\n",
    "        for m in range(len(name_allfile)):\n",
    "            divide_and_store(name_allfile[m])\n",
    "            #break\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "getall_subdir(source_bbc)\n",
    "getall_subdir(source_bbcsport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add GOT to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_got = \"corpus/got/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "allgot_files = glob.glob(source_got + \"*.txt\")\n",
    "for k in range(len(allgot_files)):\n",
    "    divide_and_store(allgot_files[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add UCI news to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_uci = \"corpus/uci_news/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alluci_news = glob.glob(source_uci + \"*.txt\")\n",
    "for k in range(len(alluci_news)):\n",
    "    divide_and_storeuci(alluci_news[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
